{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-mcOl0JY8Xg"
   },
   "source": [
    "# Day 4 - Google Search grounding with the Gemini API\n",
    "\n",
    "Welcome back to the Kaggle 5-day Generative AI course!\n",
    "\n",
    "In this optional notebook, you will use [Google Search](https://google.com/) results with the Gemini API in a technique called grounding, where the model is connected to verifiable sources of information. Using search grounding is similar to using the RAG system you implemented earlier in the week, but the Gemini API automates a lot of it for you. The model generates Google Search queries and invokes the searches automatically, retrieving relevant data from Google's index of the web and providing links to search suggestions that support the query, so your users can verify the sources.\n",
    "\n",
    "## New in Gemini 2.0\n",
    "\n",
    "Gemini 2.0 Flash provides a generous Google Search quota as part of the [free tier](https://ai.google.dev/pricing). If you switch models back to 1.5, you will need to [enable billing](https://aistudio.google.com/apikey) to use Grounding with Google Search, or you can [try it out in AI Studio](https://aistudio.google.com/). See the [earlier versions of this notebook](https://www.kaggle.com/code/markishere/day-4-google-search-grounding?scriptVersionId=207458162) for guidance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Use Google AI Studio\n",
    "\n",
    "If you wish to try out grounding with Google Search, follow this section to try it out using the AI Studio interface. Or skip ahead to the `API` section to try the feature here in your notebook.\n",
    "\n",
    "### Open AI Studio\n",
    "\n",
    "Start by going to [AI Studio](https://aistudio.google.com/prompts/new_chat). You should be in the \"New chat\" interface.\n",
    "\n",
    "Search Grounding is best with `gemini-2.0-flash`, but try out `gemini-1.5-flash` too.\n",
    "\n",
    "![New chat in AI Studio](https://storage.googleapis.com/generativeai-downloads/kaggle/ais-newchat.png)\n",
    "\n",
    "### Ask a question\n",
    "\n",
    "Now enter a prompt into the chat interface. Try asking something that is timely and might require recent information to answer, like a recent sport score. For this query, grounding will be **disabled** by default.\n",
    "\n",
    "This screenshow shows the response for `What were the top halloween costumes this year?`. Every execution will be different but typically the model talks about 2023, and hedges its responses saying it doesn't have access to specific information resulting in a general comment, rather than specific answers.\n",
    "\n",
    "![Sample question-answer pair without grounding](https://storage.googleapis.com/generativeai-downloads/kaggle/cricket-ungrounded.png)\n",
    "\n",
    "### Enable grounding\n",
    "\n",
    "On the right-hand sidebar, under the `Tools` section. Find and enable the `Grounding` option.\n",
    "\n",
    "![Enable grounding button](https://storage.googleapis.com/generativeai-downloads/kaggle/enable-grounding.png)\n",
    "\n",
    "Now re-run your question by hovering over the user prompt in the chat history, and pressing the Gemini âœ¨ icon to re-run your prompt.\n",
    "\n",
    "![Re-run prompt button](https://storage.googleapis.com/generativeai-downloads/kaggle/re-run-button.png)\n",
    "\n",
    "You should now see a response generated that references sources from Google Search.\n",
    "\n",
    "![Response with grounded sources from Google!](https://storage.googleapis.com/generativeai-downloads/kaggle/cricket-grounded.png)\n",
    "\n",
    "\n",
    "### Try your own queries\n",
    "\n",
    "Explore this interface and try some other queries. Share what works well in the [Discord](https://discord.com/channels/1101210829807956100/1303438361117069363)! You can start from [this blank template](https://aistudio.google.com/app/prompts/1FZtxKLFZIJ1p_0rICu8K2CNIF1tkAnf4) that has search grounding enabled.\n",
    "\n",
    "The remaining steps require an API key with billing enabled. They are not required to complete this course; if you have tried grounding in AI Studio you are done for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qcyq976Gbwpo"
   },
   "source": [
    "### Install packages\n",
    "Start by installing and importing the Gemini API Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:03.429757Z",
     "iopub.status.busy": "2025-04-13T02:54:03.429405Z",
     "iopub.status.idle": "2025-04-13T02:54:23.824270Z",
     "shell.execute_reply": "2025-04-13T02:54:23.822733Z",
     "shell.execute_reply.started": "2025-04-13T02:54:03.429724Z"
    },
    "id": "1ZLC4ORSbqme",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install the google-genai, google-adk and google-api-core\n",
    "!uv pip install -qU google-genai google-adk google-api-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import Markdown, HTML, display\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NO9cdffb4KR"
   },
   "source": [
    "### Set up your API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.\n",
    "\n",
    "If the `GOOGLE_API_KEY` is not available through Kaggle secrets, prompt to enter the key using `getpass` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:25.268624Z",
     "iopub.status.busy": "2025-04-13T02:54:25.267755Z",
     "iopub.status.idle": "2025-04-13T02:54:25.581265Z",
     "shell.execute_reply": "2025-04-13T02:54:25.580277Z",
     "shell.execute_reply.started": "2025-04-13T02:54:25.268587Z"
    },
    "id": "8NAmACYHb5DK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "except Exception:\n",
    "    import getpass\n",
    "    GOOGLE_API_KEY = getpass.getpass(\"Enter your Google API Key: \")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Automated retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:25.582911Z",
     "iopub.status.busy": "2025-04-13T02:54:25.582590Z",
     "iopub.status.idle": "2025-04-13T02:54:25.852185Z",
     "shell.execute_reply": "2025-04-13T02:54:25.851044Z",
     "shell.execute_reply.started": "2025-04-13T02:54:25.582877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a retry policy. The model might make multiple consecutive calls automatically\n",
    "# for a complex query, this ensures the client retries if it hits quota limits.\n",
    "from google.api_core import retry\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
    "  genai.models.Models.generate_content = retry.Retry(\n",
    "      predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `model_name` to be invoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemini-2.0-flash-lite'\n",
    "# gemini-2.5-pro-preview-03-25\n",
    "# gemini-2.0-flash\n",
    "# gemini-2.0-flash-lite\n",
    "# gemini-1.5-flash\n",
    "# gemini-1.5-flash-lite\n",
    "# gemini-1.5-pro\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `few_show_prompt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_show_prompt = \"\"\"\n",
    "         Hello, You are an NBA analyst with deep expertise in NBA statistics.\n",
    "            1. You are required to answer questions about 2025 playoff teams\n",
    "            2. If you are not able to find the information, respond with \"I don't know\n",
    "            3. Provides citations with source and links for the answer\"\n",
    "         \"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the `few_show_prompt` without using Grounding\n",
    "Here we will append a `question` to the `few_show_prompt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:25.853969Z",
     "iopub.status.busy": "2025-04-13T02:54:25.853523Z",
     "iopub.status.idle": "2025-04-13T02:54:26.440824Z",
     "shell.execute_reply": "2025-04-13T02:54:26.439776Z",
     "shell.execute_reply.started": "2025-04-13T02:54:25.853933Z"
    },
    "id": "JZmdaOlVfCgd",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know. As of today, November 2, 2023, the 2025 NBA playoff teams are yet to be determined. The 2024-2025 NBA season hasn't even started, and predicting playoff teams that far in advance is highly speculative due to player development, trades, injuries, and other unforeseen circumstances.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the 2025 NBA playoff teams?\"\n",
    "\n",
    "# Ask for information without search grounding.\n",
    "response = client.models.generate_content(\n",
    "    model=model_name,\n",
    "    contents=few_show_prompt+question)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rvre6fOrcHi2"
   },
   "source": [
    "## Use search grounding\n",
    "\n",
    "### Make a request\n",
    "\n",
    "To enable search grounding, you specify it as a tool: `google_search`. Like other tools, this is supplied as a parameter in `GenerateContentConfig`, and can be passed to `generate_content` calls as well as `chats.create` (for all chat turns) or `chat.send_message` (for specific turns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemini-2.0-flash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:26.443748Z",
     "iopub.status.busy": "2025-04-13T02:54:26.443431Z",
     "iopub.status.idle": "2025-04-13T02:54:27.826013Z",
     "shell.execute_reply": "2025-04-13T02:54:27.824975Z",
     "shell.execute_reply.started": "2025-04-13T02:54:26.443716Z"
    },
    "id": "i7jqG3nww6kU",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The 2025 NBA playoffs are underway! Here's a look at the teams involved and some expert predictions:\n",
       "\n",
       "**Teams in the 2025 Playoffs:**\n",
       "\n",
       "Based on the search results, the following teams are participating in the 2025 NBA Playoffs \\[1]:\n",
       "\n",
       "*   **Eastern Conference:**\n",
       "    *   Boston Celtics\n",
       "    *   Cleveland Cavaliers\n",
       "    *   New York Knicks\n",
       "    *   Milwaukee Bucks\n",
       "    *   Indiana Pacers\n",
       "    *   Detroit Pistons\n",
       "    *   Orlando Magic\n",
       "    *   Miami Heat\n",
       "*   **Western Conference:**\n",
       "    *   Oklahoma City Thunder\n",
       "    *   Houston Rockets\n",
       "    *   Los Angeles Lakers\n",
       "    *   Denver Nuggets\n",
       "    *   Los Angeles Clippers\n",
       "    *   Minnesota Timberwolves\n",
       "    *   Golden State Warriors\n",
       "    *   Memphis Grizzlies\n",
       "\n",
       "**First Round Matchups:**\n",
       "\n",
       "Several sources mention specific first-round matchups \\[2, 6]:\n",
       "\n",
       "*   Oklahoma City Thunder vs. Memphis Grizzlies\n",
       "*   Boston Celtics vs. Orlando Magic\n",
       "*   Cleveland Cavaliers vs. Miami Heat\n",
       "*   Houston Rockets vs. Golden State Warriors\n",
       "*   Los Angeles Lakers vs. Minnesota Timberwolves\n",
       "*   Los Angeles Clippers vs. Denver Nuggets\n",
       "*   Milwaukee Bucks vs. Indiana Pacers\n",
       "*   New York Knicks vs. Detroit Pistons\n",
       "\n",
       "**Expert Predictions**\n",
       "\n",
       "Many experts predict the Oklahoma City Thunder will win the NBA Finals over the Boston Celtics \\[4]. One source suggests an upset with the Detroit Pistons beating the New York Knicks \\[2].\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now re-run the same query with search grounding enabled.\n",
    "config_with_google_search = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    ")\n",
    "\n",
    "def query_with_grounding():\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=few_show_prompt+question,\n",
    "        config=config_with_google_search,\n",
    "    )\n",
    "    return response.candidates[0]\n",
    "\n",
    "\n",
    "rc = query_with_grounding()\n",
    "Markdown(rc.content.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.tools import google_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the citations function declaration for the model\n",
    "citations_function = {\n",
    "    \"name\": \"citations_for_answer\",\n",
    "    \"description\": \"Provides citations for the answer.\",\n",
    "     \"parameters\": {\n",
    "         \"type\": \"object\",\n",
    "         \"properties\": {\n",
    "             \"source\": {\n",
    "                 \"type\": \"string\",\n",
    "                 \"description\": \"source of the information\",\n",
    "             },\n",
    "             \"datetime\": {\n",
    "                 \"type\": \"string\",\n",
    "                 \"description\": \"Date and Time of the published information.\",\n",
    "             },\n",
    "            \"url\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"URL of the source.\",\n",
    "            },\n",
    "         },\n",
    "         \"required\": [\"source\", \"datetime\", \"url\"],\n",
    "     },\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = types.Tool(function_declarations=[citations_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent = LlmAgent(\n",
    "    model=model_name,\n",
    "    name='search_agent',\n",
    "    instruction=few_show_prompt+question,\n",
    "    tools=[google_search],\n",
    "    output_key=\"data\"\n",
    ")\n",
    "citation_prompt = \"You're an expert in providing citations for the answer, Process data from state key 'data'.\"\n",
    "citation_agent = LlmAgent(\n",
    "    model=model_name,\n",
    "    name='citation_agent',\n",
    "    instruction=citation_prompt,\n",
    ")\n",
    "main_agent = LlmAgent(\n",
    "    name=\"main_agent\",\n",
    "    model=model_name,\n",
    "    instruction=\"You're an agent orchestrator, you can use your agents to get to the end result and serve the user\",\n",
    "    description=\"Main Agent\",\n",
    "    sub_agents=[ # Assign sub_agents here\n",
    "        search_agent,\n",
    "        citation_agent\n",
    "    ]    \n",
    "    # tools=[agent_tool.AgentTool(agent=search_agent), agent_tool.AgentTool(agent=citation_agent)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-45 (_asyncio_thread_main):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pparava1/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Users/pparava1/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/runners.py\", line 138, in _asyncio_thread_main\n",
      "    asyncio.run(_invoke_run_async())\n",
      "  File \"/Users/pparava1/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pparava1/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pparava1/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/runners.py\", line 126, in _invoke_run_async\n",
      "    async for event in self.run_async(\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/runners.py\", line 197, in run_async\n",
      "    async for event in invocation_context.agent.run_async(invocation_context):\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/agents/base_agent.py\", line 141, in run_async\n",
      "    async for event in self._run_async_impl(ctx):\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/agents/llm_agent.py\", line 232, in _run_async_impl\n",
      "    async for event in self._llm_flow.run_async(ctx):\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 231, in run_async\n",
      "    async for event in self._run_one_step_async(invocation_context):\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 257, in _run_one_step_async\n",
      "    async for llm_response in self._call_llm_async(\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 470, in _call_llm_async\n",
      "    async for llm_response in llm.generate_content_async(\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/models/google_llm.py\", line 86, in generate_content_async\n",
      "    self._api_backend,\n",
      "    ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pparava1/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/functools.py\", line 998, in __get__\n",
      "    val = self.func(instance)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/models/google_llm.py\", line 161, in _api_backend\n",
      "    return 'vertex' if self.api_client.vertexai else 'ml_dev'\n",
      "                       ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pparava1/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/functools.py\", line 998, in __get__\n",
      "    val = self.func(instance)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/adk/models/google_llm.py\", line 155, in api_client\n",
      "    return Client(\n",
      "           ^^^^^^^\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/genai/client.py\", line 200, in __init__\n",
      "    self._api_client = self._get_api_client(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/genai/client.py\", line 245, in _get_api_client\n",
      "    return BaseApiClient(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/pparava1/git-ws/learning/gcp/google-gemini-gen-ai-samples/.venv/lib/python3.12/site-packages/google/genai/_api_client.py\", line 425, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"test_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\"\n",
    "\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types \n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "# Session and Runner\n",
    "session_service = InMemorySessionService()\n",
    "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
    "runner = Runner(agent=main_agent, app_name=APP_NAME, session_service=session_service)\n",
    "\n",
    "\n",
    "# Agent Interaction\n",
    "def call_agent(query):\n",
    "  content = types.Content(role='user', parts=[types.Part(text=few_show_prompt+question)])\n",
    "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
    "\n",
    "  for event in events:\n",
    "      if event.is_final_response():\n",
    "          final_response = event.content.parts[0].text\n",
    "          print(\"Agent Response: \", final_response)\n",
    "\n",
    "call_agent(\"callback example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Function calling to request citations with links\n",
    "\n",
    "Create function declaration for citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google-adk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {'google_search': {}},\n",
    "    {'function_declarations': [citations_function]} # not defined here.\n",
    "]\n",
    "\n",
    "config = types.LiveConnectConfig(\n",
    "    response_modalities=[\"TEXT\"],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "async with client.aio.live.connect(model=model_name, config=config) as session:\n",
    "    await session.send_client_content(\n",
    "        turns={\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": few_show_prompt+question,\n",
    "        },\n",
    "        turn_complete=True,\n",
    "    )\n",
    "\n",
    "    async for response in session.receive():\n",
    "        print(response.tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:27.827839Z",
     "iopub.status.busy": "2025-04-13T02:54:27.827513Z",
     "iopub.status.idle": "2025-04-13T02:54:27.834265Z",
     "shell.execute_reply": "2025-04-13T02:54:27.833171Z",
     "shell.execute_reply.started": "2025-04-13T02:54:27.827807Z"
    },
    "id": "2P7IYMcvxtcy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "while not rc.grounding_metadata.grounding_supports or not rc.grounding_metadata.grounding_chunks:\n",
    "    # If incomplete grounding data was returned, retry.\n",
    "    rc = query_with_grounding()\n",
    "\n",
    "chunks = rc.grounding_metadata.grounding_chunks\n",
    "for chunk in chunks:\n",
    "    print(f'{chunk.web.title}: {chunk.web.uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the configurations for google_search and function calling.\n",
    "# Configuration for google_search only.\n",
    "config_with_google_search = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    ")\n",
    "\n",
    "# Configuration for function calling only.\n",
    "config_with_function_calling = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(google_search=types.GoogleSearch()), types.Tool(function_declarations=[citations_function])],\n",
    ")\n",
    "\n",
    "# Use the appropriate configuration based on the requirement.\n",
    "def query_with_function_calling():\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=few_show_prompt+question,\n",
    "        config=config_with_function_calling,\n",
    "    )\n",
    "    return response.candidates[0]\n",
    "\n",
    "# Example usage for function calling.\n",
    "rc = query_with_function_calling()\n",
    "Markdown(rc.content.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJc_0FFBgoiJ"
   },
   "source": [
    "### Response metadata\n",
    "\n",
    "When search grounding is used, the model returns extra metadata that includes links to search suggestions, supporting documents and information on how the supporting documents were used.\n",
    "\n",
    "Each \"grounding chunk\" represents information retrieved from Google Search that was used in the grounded generation request. Following the URI will take you to the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziYb2Fkjzwwx"
   },
   "source": [
    "As part of the response, there is a standalone styled HTML content block that you use to link back to relevant search suggestions related to the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:27.836131Z",
     "iopub.status.busy": "2025-04-13T02:54:27.835709Z",
     "iopub.status.idle": "2025-04-13T02:54:27.857410Z",
     "shell.execute_reply": "2025-04-13T02:54:27.856334Z",
     "shell.execute_reply.started": "2025-04-13T02:54:27.836075Z"
    },
    "id": "DQAgIGJmfxqC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "HTML(rc.grounding_metadata.search_entry_point.rendered_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJpqJopp0H0M"
   },
   "source": [
    "The `grounding_supports` in the metadata provide a way for you to correlate the grounding chunks used to the generated output text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:27.859377Z",
     "iopub.status.busy": "2025-04-13T02:54:27.858922Z",
     "iopub.status.idle": "2025-04-13T02:54:27.872012Z",
     "shell.execute_reply": "2025-04-13T02:54:27.870240Z",
     "shell.execute_reply.started": "2025-04-13T02:54:27.859296Z"
    },
    "id": "sHg9Yq9U0r89",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "supports = rc.grounding_metadata.grounding_supports\n",
    "for support in supports:\n",
    "    pprint(support.to_json_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkQAGyi87FGn"
   },
   "source": [
    "These supports can be used to highlight text in the response, or build tables of footnotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:27.874429Z",
     "iopub.status.busy": "2025-04-13T02:54:27.874029Z",
     "iopub.status.idle": "2025-04-13T02:54:27.891691Z",
     "shell.execute_reply": "2025-04-13T02:54:27.890287Z",
     "shell.execute_reply.started": "2025-04-13T02:54:27.874384Z"
    },
    "id": "9_dEINt43C62",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "markdown_buffer = io.StringIO()\n",
    "\n",
    "# Print the text with footnote markers.\n",
    "markdown_buffer.write(\"Supported text:\\n\\n\")\n",
    "for support in supports:\n",
    "    markdown_buffer.write(\" * \")\n",
    "    markdown_buffer.write(\n",
    "        rc.content.parts[0].text[support.segment.start_index : support.segment.end_index]\n",
    "    )\n",
    "\n",
    "    for i in support.grounding_chunk_indices:\n",
    "        chunk = chunks[i].web\n",
    "        markdown_buffer.write(f\"<sup>[{i+1}]</sup>\")\n",
    "\n",
    "    markdown_buffer.write(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# And print the footnotes.\n",
    "markdown_buffer.write(\"Citations:\\n\\n\")\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    markdown_buffer.write(f\"{i}. [{chunk.web.title}]({chunk.web.uri})\\n\")\n",
    "\n",
    "\n",
    "Markdown(markdown_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search with tools\n",
    "\n",
    "In this example, you'll use enable the Google Search grounding tool and the code generation tool across two steps. In the first step, the model will use Google Search to find the requested information and then in the follow-up question, it generates code to plot the results.\n",
    "\n",
    "This usage includes textual, visual and code parts, so first define a function to help visualise these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:27.893940Z",
     "iopub.status.busy": "2025-04-13T02:54:27.893260Z",
     "iopub.status.idle": "2025-04-13T02:54:27.905083Z",
     "shell.execute_reply": "2025-04-13T02:54:27.903995Z",
     "shell.execute_reply.started": "2025-04-13T02:54:27.893885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "def show_response(response):\n",
    "    for p in response.candidates[0].content.parts:\n",
    "        if p.text:\n",
    "            display(Markdown(p.text))\n",
    "        elif p.inline_data:\n",
    "            display(Image(p.inline_data.data))\n",
    "        else:\n",
    "            print(p.to_json_dict())\n",
    "    \n",
    "        display(Markdown('----'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start a chat asking for some information. Here you provide the Google Search tool so that the model can look up data from Google's Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:27.906838Z",
     "iopub.status.busy": "2025-04-13T02:54:27.906515Z",
     "iopub.status.idle": "2025-04-13T02:54:30.226571Z",
     "shell.execute_reply": "2025-04-13T02:54:30.225499Z",
     "shell.execute_reply.started": "2025-04-13T02:54:27.906806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config_with_search = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "chat = client.chats.create(model='gemini-2.0-flash')\n",
    "\n",
    "response = chat.send_message(\n",
    "    message=prompt,\n",
    "    config=config_with_search,\n",
    ")\n",
    "\n",
    "show_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the chat, now ask the model to convert the data into a chart. The `code_execution` tool is able to generate code to draw charts, execute that code and return the image. You can see the executed code in the `executable_code` part of the response.\n",
    "\n",
    "Combining results from Google Search with tools like live plotting can enable very powerful use cases that require very little code to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:30.228147Z",
     "iopub.status.busy": "2025-04-13T02:54:30.227816Z",
     "iopub.status.idle": "2025-04-13T02:54:35.954075Z",
     "shell.execute_reply": "2025-04-13T02:54:35.952902Z",
     "shell.execute_reply.started": "2025-04-13T02:54:30.228113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config_with_code = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "response = chat.send_message(\n",
    "    message=\"Now plot this as a seaborn chart. Break out the medals too.\",\n",
    "    config=config_with_code,\n",
    ")\n",
    "\n",
    "show_response(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-4-google-search-grounding.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
