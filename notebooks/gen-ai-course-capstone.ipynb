{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: 2025 NBA Playoffs Q&A\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- This project demonstrates the use of advanced AI capabilities to answer questions about the **2025 NBA Playoffs**. \n",
    "- It leverages the Gemini API Python SDK and integrates tools like Google Search for grounding and citation generation. \n",
    "- The notebook showcases how to interact with AI models, evaluate their responses, and provide grounded, citation-backed answers to user queries.\n",
    "\n",
    "---\n",
    "\n",
    "## Capabilities Demonstrated\n",
    "\n",
    "### 1. **Package Installation and Setup**\n",
    "- Installs required packages (`google-genai`, `google-api-core`).\n",
    "- Imports necessary modules and initializes the Gemini API client using an API key.\n",
    "\n",
    "### 2. **Automated Retry Mechanism**\n",
    "- Implements a retry policy to handle quota limits or temporary API errors during model calls.\n",
    "\n",
    "### 3. **Model Invocation**\n",
    "- Defines and uses the `gemini-2.0-flash` model for generating responses.\n",
    "- Supports both few-shot prompting and grounding with external tools like Google Search.\n",
    "\n",
    "### 4. **Few-Shot Prompting**\n",
    "- Constructs a detailed prompt to simulate an NBA analyst answering questions about the 2025 NBA Playoffs.\n",
    "- Handles cases where the model cannot find information by responding with \"I don't know.\"\n",
    "\n",
    "### 5. **Grounded Responses**\n",
    "- Enhances responses by integrating search grounding using Google Search.\n",
    "- Uses Context caching for 5 minutes for cost efficiency\n",
    "- Provides citations with sources and links for the generated answers.\n",
    "\n",
    "### 6. **Evaluation of AI Responses**\n",
    "- Defines a structured evaluation prompt to assess the quality of AI-generated responses.\n",
    "- Evaluates responses based on criteria like instruction following, groundedness, conciseness, and fluency.\n",
    "- Assigns ratings using a predefined rubric and provides step-by-step explanations for the evaluation.\n",
    "\n",
    "### 7. **Citation Generation**\n",
    "- Processes data to generate citations for answers, ensuring transparency and reliability.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Being Solved\n",
    "\n",
    "The notebook addresses the challenge of providing accurate, grounded, and citation-backed answers to questions about the 2025 NBA Playoffs. It demonstrates how to:\n",
    "- Use AI models to generate informative responses.\n",
    "- Enhance responses with external data sources for grounding.\n",
    "- Evaluate and improve the quality of AI-generated content.\n",
    "- Provide users with reliable and transparent answers, including citations for verification.\n",
    "\n",
    "This workflow is particularly useful for scenarios requiring expert-level insights, grounded information, and high-quality content generation, such as sports analysis, journalism, and research.\n",
    "\n",
    "---\n",
    "\n",
    "## Gen AI Capabilities used\n",
    "\n",
    "- Few-shot prompting\n",
    "- Grounding\n",
    "- Gen AI evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Run Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qcyq976Gbwpo"
   },
   "source": [
    "### 1. Install packages\n",
    "Start by installing and importing the Gemini API Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:03.429757Z",
     "iopub.status.busy": "2025-04-13T02:54:03.429405Z",
     "iopub.status.idle": "2025-04-13T02:54:23.824270Z",
     "shell.execute_reply": "2025-04-13T02:54:23.822733Z",
     "shell.execute_reply.started": "2025-04-13T02:54:03.429724Z"
    },
    "id": "1ZLC4ORSbqme",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install the google-genai, google-adk and google-api-core\n",
    "!uv pip install -qU google-genai google-adk google-api-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import Markdown, HTML, display\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NO9cdffb4KR"
   },
   "source": [
    "### 3. Set up your API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.\n",
    "\n",
    "If the `GOOGLE_API_KEY` is not available through Kaggle secrets, prompt to enter the key using `getpass` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:25.268624Z",
     "iopub.status.busy": "2025-04-13T02:54:25.267755Z",
     "iopub.status.idle": "2025-04-13T02:54:25.581265Z",
     "shell.execute_reply": "2025-04-13T02:54:25.580277Z",
     "shell.execute_reply.started": "2025-04-13T02:54:25.268587Z"
    },
    "id": "8NAmACYHb5DK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "except Exception:\n",
    "    import getpass\n",
    "    GOOGLE_API_KEY = getpass.getpass(\"Enter your Google API Key: \")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup Automated retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:25.582911Z",
     "iopub.status.busy": "2025-04-13T02:54:25.582590Z",
     "iopub.status.idle": "2025-04-13T02:54:25.852185Z",
     "shell.execute_reply": "2025-04-13T02:54:25.851044Z",
     "shell.execute_reply.started": "2025-04-13T02:54:25.582877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a retry policy. The model might make multiple consecutive calls automatically\n",
    "# for a complex query, this ensures the client retries if it hits quota limits.\n",
    "from google.api_core import retry\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
    "  genai.models.Models.generate_content = retry.Retry(\n",
    "      predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define `model_name` to be invoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemini-2.0-flash'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define `few_show_prompt` and `question`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_show_prompt = \"\"\"\n",
    "       Hello, You are an NBA analyst with deep expertise in NBA statistics.\n",
    "         1. You are required to answer questions about the 2025 playoff teams.\n",
    "         2. If you are not able to find the information, respond with \"I don't know.\"\n",
    "         3. Provide citations with sources and links in the answer.\n",
    "       \"\"\" \n",
    "\n",
    "question = \"Which teams are in the 2025 NBA playoffs?\"\n",
    "\n",
    "input_content = few_show_prompt + question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test the `few_show_prompt` without using Grounding\n",
    "Here we will append a `question` to the `few_show_prompt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:25.853969Z",
     "iopub.status.busy": "2025-04-13T02:54:25.853523Z",
     "iopub.status.idle": "2025-04-13T02:54:26.440824Z",
     "shell.execute_reply": "2025-04-13T02:54:26.439776Z",
     "shell.execute_reply.started": "2025-04-13T02:54:25.853933Z"
    },
    "id": "JZmdaOlVfCgd",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### As an NBA analyst, I'd love to tell you which teams are in the 2025 NBA playoffs! However, **I don't know** that information yet. The 2025 playoffs haven't happened, and predicting the exact teams that will qualify so far in advance is impossible due to player development, injuries, trades, and other unforeseen circumstances.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Ask for information without search grounding.\n",
    "response = client.models.generate_content(\n",
    "    model=model_name,\n",
    "    contents=input_content)\n",
    "\n",
    "few_shot_response_text = response.text\n",
    "# Display the response\n",
    "# Format the response as a Markdown cell\n",
    "few_shot_formatted_response = f\"### {few_shot_response_text}\"\n",
    "Markdown(few_shot_formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rvre6fOrcHi2"
   },
   "source": [
    "### 8. Use Grounding grounding `google_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T02:54:26.443748Z",
     "iopub.status.busy": "2025-04-13T02:54:26.443431Z",
     "iopub.status.idle": "2025-04-13T02:54:27.826013Z",
     "shell.execute_reply": "2025-04-13T02:54:27.824975Z",
     "shell.execute_reply.started": "2025-04-13T02:54:26.443716Z"
    },
    "id": "i7jqG3nww6kU",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### The 2025 NBA playoffs are underway, and the bracket is officially set. Here are the teams that have made it to the first round:\n",
       "\n",
       "**Eastern Conference**\n",
       "\n",
       "*   (1) Cleveland Cavaliers vs. (8) Miami Heat\n",
       "*   (2) Boston Celtics vs. (7) Orlando Magic\n",
       "*   (3) New York Knicks vs. (6) Detroit Pistons\n",
       "*   (4) Indiana Pacers vs. (5) Milwaukee Bucks\n",
       "\n",
       "**Western Conference**\n",
       "\n",
       "*   (1) Oklahoma City Thunder vs. (8) Memphis Grizzlies\n",
       "*   (2) Houston Rockets vs. (7) Golden State Warriors\n",
       "*   (3) Los Angeles Lakers vs. (6) Minnesota Timberwolves\n",
       "*   (4) Denver Nuggets vs. (5) Los Angeles Clippers\n",
       "\n",
       "Sources:\n",
       "\n",
       "*   [https://www.cbssports.com/nba/news/2025-nba-playoff-bracket-first-round-matchups-schedule-game-times-as-heat-and-grizzlies-get-final-spots/](https://www.cbssports.com/nba/news/2025-nba-playoff-bracket-first-round-matchups-schedule-game-times-as-heat-and-grizzlies-get-final-spots/)\n",
       "*   [https://www.youtube.com/watch?v=BwXzs5GzK9I](https://www.youtube.com/watch?v=BwXzs5GzK9I)\n",
       "*   [https://en.wikipedia.org/wiki/2025_NBA_playoffs](https://en.wikipedia.org/wiki/2025_NBA_playoffs)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now re-run the same query with search grounding enabled.\n",
    "\n",
    "config_with_google_search = types.GenerateContentConfig(tools=[types.Tool(google_search=types.GoogleSearch())])\n",
    "\n",
    "def query_with_grounding():\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=input_content,\n",
    "        config=config_with_google_search,\n",
    "    )\n",
    "    return response.candidates[0]\n",
    "\n",
    "\n",
    "rc = query_with_grounding()\n",
    "grounded_response_text = rc.content.parts[0].text\n",
    "grounded_formatted_response = f\"### {grounded_response_text}\"\n",
    "Markdown(grounded_formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Gen AI Evaluation\n",
    "\n",
    "Evaluate the quality of the response generated by the AI model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "# Define the evaluation prompt\n",
    "SUMMARY_PROMPT = \"\"\"\\\n",
    "# Instruction\n",
    "You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\n",
    "We will provide you with the user input and an AI-generated responses.\n",
    "You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
    "You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric.\n",
    "\n",
    "# Evaluation\n",
    "## Metric Definition\n",
    "You will be assessing summarization quality, which measures the overall ability to summarize text. Pay special attention to length constraints, such as in X words or in Y sentences. \n",
    "The instruction for performing a summarization task and the context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. \n",
    "The response should not contain information that is not present in the context.\n",
    "\n",
    "## Criteria\n",
    "Instruction following: The response demonstrates a clear understanding of the summarization task instructions, satisfying all of the instruction's requirements.\n",
    "Groundedness: The response contains information included only in the context. The response does not reference any outside information.\n",
    "Conciseness: The response summarizes the relevant details in the original text without a significant loss in key information without being too verbose or terse.\n",
    "Fluency: The response is well-organized and easy to read.\n",
    "\n",
    "## Rating Rubric\n",
    "5: (Very good). The summary follows instructions, is grounded, is concise, and fluent.\n",
    "4: (Good). The summary follows instructions, is grounded, concise, and fluent.\n",
    "3: (Ok). The summary mostly follows instructions, is grounded, but is not very concise and is not fluent.\n",
    "2: (Bad). The summary is grounded, but does not follow the instructions.\n",
    "1: (Very bad). The summary is not grounded.\n",
    "\n",
    "## Evaluation Steps\n",
    "STEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.\n",
    "STEP 2: Score based on the rubric.\n",
    "\n",
    "# User Inputs and AI-generated Response\n",
    "## User Inputs\n",
    "\n",
    "### Prompt\n",
    "{prompt}\n",
    "\n",
    "## AI-generated Response\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "# Define a structured enum class to capture the result.\n",
    "class SummaryRating(enum.Enum):\n",
    "  VERY_GOOD = '5'\n",
    "  GOOD = '4'\n",
    "  OK = '3'\n",
    "  BAD = '2'\n",
    "  VERY_BAD = '1'\n",
    "\n",
    "\n",
    "def eval_summary(prompt, ai_response):\n",
    "  \"\"\"Evaluate the generated summary against the prompt used.\"\"\"\n",
    "\n",
    "  chat = client.chats.create(\n",
    "    model=model_name,\n",
    "    config=config_with_google_search)\n",
    "\n",
    "  # Generate the full text response.\n",
    "  response = chat.send_message(\n",
    "      message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n",
    "  )\n",
    "  verbose_eval = response.text\n",
    "\n",
    "  # Coerce into the desired structure.\n",
    "  structured_output_config = types.GenerateContentConfig(\n",
    "      response_mime_type=\"text/x.enum\",\n",
    "      response_schema=SummaryRating,\n",
    "  )\n",
    "  response = chat.send_message(\n",
    "      message=\"Convert the final score.\",\n",
    "      config=structured_output_config,\n",
    "  )\n",
    "  structured_eval = response.parsed\n",
    "\n",
    "  return verbose_eval, structured_eval\n",
    "\n",
    "\n",
    "text_eval, struct_eval = eval_summary(prompt=input_content, ai_response=grounded_response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Evaluation Steps\n",
       "\n",
       "1: STEP 1: The response provided an accurate list of teams in the 2025 NBA playoffs. The links provided are relevant to the answer.\n",
       "\n",
       "2: STEP 2: The response follows instructions, is grounded, is concise, and fluent.\n",
       "\n",
       "4: Rating: 5\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format the response by steps\n",
    "def format_steps(steps):\n",
    "    formatted_steps = []\n",
    "    for i, step in enumerate(steps.split('\\n')):\n",
    "        if step.strip():\n",
    "            formatted_steps.append(f\"{i + 1}: {step.strip()}\\n\")\n",
    "    return \"\\n\".join(formatted_steps)\n",
    "eval_formatted_steps = format_steps(text_eval)\n",
    "eval_formatted_response = f\"### Evaluation Steps\\n\\n{eval_formatted_steps}\"\n",
    "Markdown(eval_formatted_response)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-4-google-search-grounding.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
